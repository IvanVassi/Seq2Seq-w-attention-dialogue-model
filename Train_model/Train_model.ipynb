{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "datafile = os.path.join(corpus_name, \"movie_lines.txt\")\n",
    "\n",
    "with open(datafile, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for line in lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(corpus_name, \"formatted_movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocess data - lower</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what s a synonym for throbbing ?',\n",
       " 'sarah lawrence is on the other side of the country .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[456]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Split Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [[pair[0].split(), pair[1].split()] for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Выкинем слишком длинные предложения</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 16\n",
    "pairs = [pair for pair in pairs if len(pair[0]) <= min_length and len(pair[1]) <= min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126591"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train Test Split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "random.shuffle(pairs)\n",
    "idx = int(len(pairs) * test_size)\n",
    "\n",
    "train_pairs, test_pairs = pairs[idx:], pairs[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113932, 12659)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Count words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter()\n",
    "\n",
    "for pair in train_pairs:\n",
    "    for word in pair[0]:\n",
    "        word_count[word] += 1\n",
    "    for word in pair[1]:\n",
    "        word_count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word to Id</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 10\n",
    "\n",
    "\n",
    "pad_idx = 0\n",
    "unk_idx = 1\n",
    "sos_idx = 2\n",
    "eos_idx = 3\n",
    "\n",
    "word2id = {\n",
    "    \"<pad>\": pad_idx,\n",
    "    \"<unk>\": unk_idx,\n",
    "    \"<sos>\": sos_idx,\n",
    "    \"<eos>\": eos_idx,\n",
    "}\n",
    "\n",
    "i = 4\n",
    "for word, count in word_count.items():\n",
    "    if count >= min_freq:\n",
    "        word2id[word] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = [], []\n",
    "    \n",
    "for pair in train_pairs:\n",
    "    train_data.append([\n",
    "        [word2id.get(word, unk_idx) for word in pair[0]],\n",
    "        [word2id.get(word, unk_idx) for word in pair[1]],\n",
    "    ])\n",
    "    \n",
    "for pair in test_pairs:\n",
    "    test_data.append([\n",
    "        [word2id.get(word, unk_idx) for word in pair[0]],\n",
    "        [word2id.get(word, unk_idx) for word in pair[1]],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 526, 167, 30, 81, 825, 9, 9, 9],\n",
       " [1655, 9, 9, 3888, 37, 23, 123, 46, 1156, 20]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get Batch</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sequences, pad_idx, max_length=None):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequences: list of list of tokens\n",
    "    '''\n",
    "    if max_length is None:\n",
    "        max_length = max(map(len, sequences))\n",
    "    \n",
    "    return [seq + [pad_idx]*(max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "def get_batch(batch_size, train):\n",
    "    if train:\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = test_data\n",
    "        \n",
    "    rand_ids = np.random.randint(0, len(data), batch_size)\n",
    "    \n",
    "    source = [data[idx][0] for idx in rand_ids]\n",
    "    target = [data[idx][1] for idx in rand_ids]\n",
    "    \n",
    "    target_in  = [[sos_idx] + sequence for sequence in target]\n",
    "    target_out = [sequence + [eos_idx] for sequence in target]\n",
    "    \n",
    "    source     = padding(source, pad_idx)\n",
    "    target_in  = padding(target_in, pad_idx)\n",
    "    target_out = padding(target_out, pad_idx)\n",
    "    \n",
    "    source     = torch.LongTensor(source).to(device)\n",
    "    target_in  = torch.LongTensor(target_in).to(device)\n",
    "    target_out = torch.LongTensor(target_out).to(device)\n",
    "    \n",
    "    return source, target_in, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target_in, target_out = get_batch(32, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 15]), torch.Size([32, 17]), torch.Size([32, 17]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.size(), target_in.size(), target_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, padding_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        self.lstm      = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, batch_words):\n",
    "        '''\n",
    "        Inputs:\n",
    "            batch_words: (batch x source_len)\n",
    "        '''\n",
    "        \n",
    "        #(batch x source_len) -> (batch x source_len x emb_size)\n",
    "        embedded = self.embedding(batch_words)\n",
    "        \n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return output\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_output, encoder_mask):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: (batch x hidden_size)\n",
    "            encoder_output: (batch x source_len x hidden_size)\n",
    "        '''\n",
    "        hidden = self.linear(hidden)\n",
    "        hidden = hidden.unsqueeze(2)\n",
    "        alphas = encoder_output.matmul(hidden)\n",
    "        \n",
    "        if encoder_mask is not None:\n",
    "            alphas[encoder_mask] = -1e16\n",
    "            \n",
    "        scores = F.softmax(alphas, dim=1)\n",
    "        c = (scores * encoder_output).sum(dim=1)\n",
    "        return c\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, padding_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
    "        self.attn      = Attention(hidden_size)\n",
    "        self.gru_cell  = nn.GRUCell(emb_size + hidden_size, hidden_size)\n",
    "        self.linear    = nn.Linear(hidden_size, vocab_size)\n",
    "  \n",
    "\n",
    "    def forward(self, batch_trans_in, encoder_output, hidden, encoder_mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            batch_trans_in: (batch x target_len)\n",
    "            encoder_output: (batch x source_len x hidden_size)\n",
    "            hidden: (batch x hidden_size)\n",
    "        '''\n",
    "        embedded  = self.embedding(batch_trans_in)\n",
    "        timesteps = embedded.size(1)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for t in range(timesteps):\n",
    "            x = embedded[:, t]\n",
    "            c = self.attn(hidden, encoder_output, encoder_mask)\n",
    "            inp = torch.cat([x, c], dim=1)\n",
    "            hidden = self.gru_cell(inp, hidden)\n",
    "            output.append(hidden)\n",
    "        \n",
    "        output = torch.stack(output, dim=1)\n",
    "        logits = self.linear(output)\n",
    "\n",
    "        return logits.view(-1, self.vocab_size), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {value: key for key, value in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(train=True):\n",
    "    source, _, _ = get_batch(1, train=True)\n",
    "    encoder_output = encoder(source)\n",
    "    \n",
    "    hidden = torch.zeros(1, hidden_size).to(device)\n",
    "    \n",
    "    generated = [sos_idx]\n",
    "    \n",
    "    for i in range(min_length):\n",
    "        \n",
    "        generated_in = torch.LongTensor([[generated[-1]]]).to(device)\n",
    "        \n",
    "        logit, hidden = decoder(generated_in, encoder_output, hidden)\n",
    "        next_idx = logit.max(1)[1][0].item()\n",
    "        generated.append(next_idx)\n",
    "        \n",
    "        if next_idx == eos_idx:\n",
    "            break\n",
    "            \n",
    "    source = [id2word.get(idx, \"unk\") for idx in source[0].tolist()]\n",
    "    target = [id2word.get(idx, \"unk\") for idx in generated if idx not in [sos_idx, eos_idx]]\n",
    "    \n",
    "    source = ' '.join(source)\n",
    "    target = ' '.join(target)\n",
    "    \n",
    "    print(source)\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size    = 256\n",
    "hidden_size = 512\n",
    "\n",
    "encoder = Encoder(len(word2id), emb_size, hidden_size, pad_idx).to(device)\n",
    "decoder = Decoder(len(word2id), emb_size, hidden_size, pad_idx).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to hide it from the i .r .s .\n",
      "rifle units choice shell honor worry glass paint print clarice party farewell clarice pie budget pedro\n"
     ]
    }
   ],
   "source": [
    "_print(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 8.689029693603516\n",
      "aren t you ever going to get married ?\n",
      "i accident t ordinary lie fooled pick pick tv nashville invitation used world slaughtered crystal nerve\n",
      "------\n",
      "did you see a girl come by here ?\n",
      "i accident t ordinary fooled woulda plank singing paint girlfriends rifle chauncey sheldrake collect bored jump\n",
      "-------\n",
      "\n",
      "Epoch: 1. Loss: 3.575674295425415\n",
      "that s none of your business .\n",
      "i m not .\n",
      "------\n",
      "the windows don t open .\n",
      "i m not .\n",
      "-------\n",
      "\n",
      "Epoch: 2. Loss: 3.3263769149780273\n",
      "that s off now .\n",
      "i m not .\n",
      "------\n",
      "he was a <unk> .\n",
      "he s a <unk> .\n",
      "-------\n",
      "\n",
      "Epoch: 3. Loss: 2.8957908153533936\n",
      "right now ?\n",
      "i m not sure .\n",
      "------\n",
      "i took care of everybody .\n",
      "you don t have to be <unk> .\n",
      "-------\n",
      "\n",
      "Epoch: 4. Loss: 2.8700754642486572\n",
      "where the hell are you from <unk> ? ? ?\n",
      "i m not . . .\n",
      "------\n",
      "some people would say you re paranoid .\n",
      "i m not .\n",
      "-------\n",
      "\n",
      "Epoch: 5. Loss: 2.616819143295288\n",
      "then we must reach her before she feels that pain .\n",
      "i know .\n",
      "------\n",
      ". . <unk> .\n",
      "i m sorry . . .\n",
      "-------\n",
      "\n",
      "Epoch: 6. Loss: 2.4934396743774414\n",
      "<unk> . <unk> . a simple <unk> .\n",
      "<unk> ?\n",
      "------\n",
      "charlie and i were talking . that s one reason i wanted to see you .\n",
      "i m sorry .\n",
      "-------\n",
      "\n",
      "Epoch: 7. Loss: 2.4196925163269043\n",
      "this is <unk> dangerous .\n",
      "i m not <unk> .\n",
      "------\n",
      "interesting .\n",
      "i m not <unk> .\n",
      "-------\n",
      "\n",
      "Epoch: 8. Loss: 2.1268599033355713\n",
      "okay . we re up at six .\n",
      "i m not .\n",
      "------\n",
      "yeah . sure . why not . call me at the <unk> beverly <unk> .\n",
      "i m <unk> .\n",
      "-------\n",
      "\n",
      "Epoch: 9. Loss: 1.9511798620224\n",
      "where s erica ? you seen her ?\n",
      "in the chest .\n",
      "------\n",
      "nothing for him ! he s being <unk> .\n",
      "you re serious ?\n",
      "-------\n",
      "\n",
      "Epoch: 10. Loss: 1.971523404121399\n",
      "this is a real nice place you got here .\n",
      "yeah it s good to see you .\n",
      "------\n",
      "i ve got a right to carry a gun if i want to .\n",
      "i m not sure ?\n",
      "-------\n",
      "\n",
      "Epoch: 11. Loss: 1.839255928993225\n",
      "i know .\n",
      "you re a great boy <unk> .\n",
      "------\n",
      "are you kidding ? i wouldn t miss this for all the tea in china .\n",
      "i m sorry .\n",
      "-------\n",
      "\n",
      "Epoch: 12. Loss: 1.7208062410354614\n",
      "i m glad you didn t get something <unk> .\n",
      "i m not <unk> .\n",
      "------\n",
      "you sure you re not thinking of him right now ?\n",
      "no .\n",
      "-------\n",
      "\n",
      "Epoch: 13. Loss: 1.6607799530029297\n",
      "what is it norman ?\n",
      "i m sorry .\n",
      "------\n",
      "you heard that <unk> ?\n",
      "yeah . . . you know who i mean . . .\n",
      "-------\n",
      "\n",
      "Epoch: 14. Loss: 1.630611538887024\n",
      "i like to think i m a master of my own destiny .\n",
      "oh come on . i know you just like it .\n",
      "------\n",
      "that really happened ?\n",
      "yeah . i was thinking .\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(15):\n",
    "    for batch_idx in range(len(train_data) // batch_size):\n",
    "        \n",
    "        source, target_in, target_out = get_batch(batch_size, train=True)\n",
    "        encoder_output = encoder(source)\n",
    "        encoder_mask = source == pad_idx\n",
    "        hidden = torch.zeros(batch_size, hidden_size).to(device)\n",
    "        logit, hidden = decoder(target_in, encoder_output, hidden, encoder_mask)\n",
    "\n",
    "        target_out = target_out.view(-1)\n",
    "        decoder_mask = target_out != pad_idx\n",
    "        decoder_mask = decoder_mask\n",
    "\n",
    "        loss = criterion(logit[decoder_mask], target_out[decoder_mask])\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(\"Epoch: %s. Loss: %s\" % (epoch, loss.item()))\n",
    "            _print(True)\n",
    "            print('------')\n",
    "            _print(False)\n",
    "            print('-------')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Hello, my name is Zuzee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepr(a):\n",
    "    a = unicodeToAscii(a)\n",
    "    a = normalizeString(a)\n",
    "    a = a.split()\n",
    "    a = [word2id.get(word, unk_idx) for word in a]\n",
    "    a = torch.LongTensor(a).to(device)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = [word2id.get(word, unk_idx) for word in a]\n",
    "#a = torch.LongTensor(a).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = a + [pad_idx]*(min_length - len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.LongTensor(a).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(source1):\n",
    "    source1 = prepr(source1)\n",
    "    seq = source1.unsqueeze(0)\n",
    "    encoder_output = encoder(seq)\n",
    "    \n",
    "    hidden = torch.zeros(1, hidden_size).to(device)\n",
    "    \n",
    "    generated = [sos_idx]\n",
    "    \n",
    "    for i in range(min_length):\n",
    "        \n",
    "        generated_in = torch.LongTensor([[generated[-1]]]).to(device)\n",
    "        \n",
    "        logit, hidden = decoder(generated_in, encoder_output, hidden)\n",
    "        next_idx = logit.max(1)[1][0].item()\n",
    "        generated.append(next_idx)\n",
    "        \n",
    "        if next_idx == eos_idx:\n",
    "            break\n",
    "            \n",
    "    seq = [id2word.get(idx, \"unk\") for idx in seq[0].tolist()]\n",
    "    target = [id2word.get(idx, \"unk\") for idx in generated if idx not in [sos_idx, eos_idx]]\n",
    "    \n",
    "    seq = ' '.join(seq)\n",
    "    target = ' '.join(target)\n",
    "    \n",
    "    print(seq)\n",
    "    print(target)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = eval(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id2word.pkl', 'wb') as f:\n",
    "        pickle.dump(id2word, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2id.pkl', 'wb') as f:\n",
    "        pickle.dump(word2id, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), 'decoder.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
